{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle Competition: Natural Language Processing with Disaster Tweets\n","metadata":{}},{"cell_type":"markdown","source":"## Description of the Problem and Data\nBriefly describe the challenge problem and NLP.  Describe the size, dimension, structure, etc., of the data.","metadata":{}},{"cell_type":"code","source":"# Import\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport string\nfrom urllib.parse import unquote\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:06.675742Z","iopub.execute_input":"2025-10-27T15:46:06.676074Z","iopub.status.idle":"2025-10-27T15:46:07.985639Z","shell.execute_reply.started":"2025-10-27T15:46:06.676049Z","shell.execute_reply":"2025-10-27T15:46:07.984892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Problem\n(from Kaggle) Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nNatural Language Processing - NLP enables computers to understand, interpret, and generate human language.  Key aspects include: Understanding context and meaning, processing text and speech, and enabling technologies for use in applications, like machine translation or voice assistants.  ","metadata":{}},{"cell_type":"markdown","source":"Sources for NLP techniques: NLTK, tensorflow, keras, and sklearn documentation. Reddit r/NLP.   ","metadata":{}},{"cell_type":"markdown","source":"### Data\nEach sample in the train and test set has the following information: The text of a tweet, a keyword from that tweet (watch for blanks!), and the location the tweet was sent from (also could be missing).\n\nWe are predicting whether a given tweet is about a real disaster or not.  If so, predict a '1', otherwise '0'.\n\nColumns: 'id', 'text', 'location', 'keyword', 'target' (in train.csv only, this denotes whether a tweet is about a real disaster or not).","metadata":{}},{"cell_type":"code","source":"# load the training dataset\ndf_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:07.987053Z","iopub.execute_input":"2025-10-27T15:46:07.987588Z","iopub.status.idle":"2025-10-27T15:46:08.027192Z","shell.execute_reply.started":"2025-10-27T15:46:07.987564Z","shell.execute_reply":"2025-10-27T15:46:08.026473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Describe the data size, dimension, structure, etc...\nprint(f\"Dimensions (Rows, Columns): {df_train.shape}\")\nprint(f\"Total elements: {df_train.size}\")\n\ndf_train.info()\nprint(\"\\nData Types per Column:\\n\", df_train.dtypes)\nprint(\"\\nFirst 3 rows:\\n\", df_train.head(3))\n\n# Check for missing values (hinted at in problem description)\nprint(\"\\nMissing values per column:\\n\", df_train.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:08.028074Z","iopub.execute_input":"2025-10-27T15:46:08.028382Z","iopub.status.idle":"2025-10-27T15:46:08.048725Z","shell.execute_reply.started":"2025-10-27T15:46:08.028352Z","shell.execute_reply":"2025-10-27T15:46:08.047684Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis\nShow a few visualizations.  Describe any data cleaning procedures.  Based on this EDA, what is the plan of analysis?","metadata":{}},{"cell_type":"markdown","source":"'keyword' is missing 61 values.  This represents a small fraction of the data so I think that just deleting the rows\n will be the best option.  This will have a negligible impact on the model's training. ","metadata":{}},{"cell_type":"code","source":"# Handle missing values from 'keyword'\ndf_train.dropna(subset=['keyword'], inplace=True)\nprint(f\"New training dataframe shape after dropping missing rows: {df_train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:08.051314Z","iopub.execute_input":"2025-10-27T15:46:08.051573Z","iopub.status.idle":"2025-10-27T15:46:08.063737Z","shell.execute_reply.started":"2025-10-27T15:46:08.051552Z","shell.execute_reply":"2025-10-27T15:46:08.062815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After accounting for the missing values in 'keyword', 'location' has 2472 missing values.  This is a large percentage of the data so I do not just want to delete it.  I'll replace the na's with a 'NONE_PROVIDED' to give them their own category and we can potentially check if this category has any predictive value.","metadata":{}},{"cell_type":"code","source":"# Handle missing values from 'location'\ndf_train['location'] = df_train['location'].fillna('NONE_PROVIDED')\nprint(\"Missing 'location' values imputed with 'NONE_PROVIDED'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:08.064863Z","iopub.execute_input":"2025-10-27T15:46:08.065335Z","iopub.status.idle":"2025-10-27T15:46:08.081501Z","shell.execute_reply.started":"2025-10-27T15:46:08.065294Z","shell.execute_reply":"2025-10-27T15:46:08.080675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the balance of the target classes using a count plot\nplt.figure(figsize=(6,5))\nsns.countplot(x='target', data=df_train)\nplt.title('Distribution of Target Class')\nplt.xlabel('Target (0: Not Disaster, 1: Disaster)')\nplt.ylabel('Count')\nplt.xticks([0,1])\nplt.grid(axis='y', alpha = 0.5)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:08.082640Z","iopub.execute_input":"2025-10-27T15:46:08.083057Z","iopub.status.idle":"2025-10-27T15:46:08.234413Z","shell.execute_reply.started":"2025-10-27T15:46:08.083021Z","shell.execute_reply":"2025-10-27T15:46:08.233449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Text Cleaning and Normalization\nRemove/replace elements that are typically irrelevant to the text's meaning.  HTML tags, URLs, Hashtags, Mentions, and Punctuation.\n\nStandardize the remaining text.  Lowercasing, tokenization (splitting text into individual words/sub-words), Stop Word removal, and Lemmatize (reduce to base form of word).","metadata":{}},{"cell_type":"code","source":"# Preprocessing Function\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    # HTML tags\n    text = re.sub(r'<.*?>', '', text)\n    # URLs\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    # Mentions and Hashtags\n    text = re.sub(r'@\\w+|#', '', text)\n    # Punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Lowercase\n    text = text.lower()\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Stop Word removal and lemmatize\n    processed_tokens = []\n    for word in tokens:\n        if word not in stop_words:\n            processed_tokens.append(lemmatizer.lemmatize(word))\n\n    return \" \".join(processed_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:08.235417Z","iopub.execute_input":"2025-10-27T15:46:08.236052Z","iopub.status.idle":"2025-10-27T15:46:08.244098Z","shell.execute_reply.started":"2025-10-27T15:46:08.236023Z","shell.execute_reply":"2025-10-27T15:46:08.243152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply preprocessing to 'text' column\ndf_train['clean_text'] = df_train['text'].apply(preprocess_text)\n\nprint(\"Text column ('text') has been cleaned and normalized into 'clean_text'.\")\nprint(\"\\nExample Original vs. Cleaned Text:\")\nprint(f\"Original: {df_train['text'].iloc[0]}\")\nprint(f\"Cleaned:  {df_train['clean_text'].iloc[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:08.245129Z","iopub.execute_input":"2025-10-27T15:46:08.245509Z","iopub.status.idle":"2025-10-27T15:46:12.458130Z","shell.execute_reply.started":"2025-10-27T15:46:08.245487Z","shell.execute_reply":"2025-10-27T15:46:12.457078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocess 'keyword' Column\ndecode the URL encodings, aggregate low frequency keywords, One-hot encode","metadata":{}},{"cell_type":"code","source":"# apply unquote to keyword\ndef unquote_keyword(keyword):\n    if pd.isna(keyword): return 'UNKNOWN'\n    return unquote(keyword)\n    \ndf_train['clean_keyword'] = df_train['keyword'].apply(unquote_keyword)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:12.458958Z","iopub.execute_input":"2025-10-27T15:46:12.459246Z","iopub.status.idle":"2025-10-27T15:46:12.472088Z","shell.execute_reply.started":"2025-10-27T15:46:12.459224Z","shell.execute_reply":"2025-10-27T15:46:12.471184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get counts of unique keywords\nkeyword_counts = df_train['clean_keyword'].value_counts()\nunique_count = len(keyword_counts)\n\nprint(keyword_counts.head())\nprint(\"-\" * 50)\nprint(keyword_counts.tail())\nprint(\"-\" * 50)\n\nprint(f\"The total number of unique keywords is: {unique_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:12.475292Z","iopub.execute_input":"2025-10-27T15:46:12.475510Z","iopub.status.idle":"2025-10-27T15:46:12.494258Z","shell.execute_reply.started":"2025-10-27T15:46:12.475492Z","shell.execute_reply":"2025-10-27T15:46:12.493433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Aggregate 'keyword'\nAggregating keywords into a 'top N' and 'other' category should help with training the model by reducing dimensionality and complexity by shrinking the number of features the model has to learn from the keyword column.  This should make a simpler, faster model with less risk of overfitting.  ","metadata":{}},{"cell_type":"code","source":"# Aggregate low-frequency keywords\nTOP_N = 75\nkeyword_counts = df_train['clean_keyword'].value_counts()\ntop_n_keywords = keyword_counts.nlargest(TOP_N).index.tolist()\n\ndf_train['agg_keyword'] = np.where(\n    df_train['clean_keyword'].isin(top_n_keywords),\n    df_train['clean_keyword'],\n    'Other'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:12.495093Z","iopub.execute_input":"2025-10-27T15:46:12.495386Z","iopub.status.idle":"2025-10-27T15:46:12.513738Z","shell.execute_reply.started":"2025-10-27T15:46:12.495366Z","shell.execute_reply":"2025-10-27T15:46:12.512938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Keyword Distribution\nkeyword_counts = df_train['agg_keyword'].value_counts().reset_index()\nkeyword_counts.columns = ['agg_keyword', 'count']\nkeyword_counts = keyword_counts.sort_values(by='count', ascending=False)\n\nplt.figure(figsize=(8,10))\nsns.barplot(x='count', y='agg_keyword', data=keyword_counts)\nplt.title('Distribution of Keywords')\nplt.xlabel('Count')\nplt.xscale('log')\nplt.ylabel('Keyword')\nplt.grid(axis='x', alpha=0.5)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:12.514628Z","iopub.execute_input":"2025-10-27T15:46:12.514922Z","iopub.status.idle":"2025-10-27T15:46:13.777943Z","shell.execute_reply.started":"2025-10-27T15:46:12.514896Z","shell.execute_reply":"2025-10-27T15:46:13.776983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-hot encode 'agg_keyword' column\nkeyword_dummies = pd.get_dummies(df_train['agg_keyword'], prefix='keyword')\n\ndf_train = pd.concat([df_train, keyword_dummies], axis=1)\n\ndf_train.drop(['keyword', 'clean_keyword', 'agg_keyword'], axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:13.778911Z","iopub.execute_input":"2025-10-27T15:46:13.779204Z","iopub.status.idle":"2025-10-27T15:46:13.790622Z","shell.execute_reply.started":"2025-10-27T15:46:13.779183Z","shell.execute_reply":"2025-10-27T15:46:13.789787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"New DataFrame Head (Showing New Keyword Columns)\")\nprint(df_train.head())\nprint(\"\\nNew DataFrame Shape\")\nprint(f\"The new shape (rows, columns) is: {df_train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:13.791404Z","iopub.execute_input":"2025-10-27T15:46:13.791721Z","iopub.status.idle":"2025-10-27T15:46:13.807920Z","shell.execute_reply.started":"2025-10-27T15:46:13.791690Z","shell.execute_reply":"2025-10-27T15:46:13.807131Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocessing the 'location' column\nThere are significant data quality issues that need to be addressed before One-hot encoding.  \n\nAliases/Variants of locations like 'USA' and 'United States'.  Non-Locations like 'worldwide' and 'everywhere'.  Foreign Characters that need correction. and single occurances that I will group together as they are unlikely to be predictive.","metadata":{}},{"cell_type":"code","source":"# create function to accomplish preprocessing steps to 'location'\ndef clean_location(loc):\n    if loc == 'NONE_PROVIDED':\n        return loc\n    try:\n        loc = loc.encode('latin1').decode('utf8')\n    except (UnicodeEncodeError, UnicodeDecodeError):\n        pass\n        \n    # lower case and strip whitespace\n    loc = loc.lower().strip()\n    \n    # remove trailing punctuation\n    loc = loc.strip(string.punctuation)\n    \n    # remove numbers and non-alphabetic noise\n    loc = re.sub(r'[\\d]', '', loc)\n    loc = re.sub(r'[^\\w\\s,\\-\\']', '', loc)\n    loc = re.sub(r'\\s+', ' ', loc).strip() # replace multiple spaces\n\n    return loc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:13.808659Z","iopub.execute_input":"2025-10-27T15:46:13.808966Z","iopub.status.idle":"2025-10-27T15:46:13.824007Z","shell.execute_reply.started":"2025-10-27T15:46:13.808945Z","shell.execute_reply":"2025-10-27T15:46:13.822991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# apply the clean_location function to the location column\ndf_train['clean_location'] = df_train['location'].apply(clean_location)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:13.824880Z","iopub.execute_input":"2025-10-27T15:46:13.825160Z","iopub.status.idle":"2025-10-27T15:46:13.865437Z","shell.execute_reply.started":"2025-10-27T15:46:13.825136Z","shell.execute_reply":"2025-10-27T15:46:13.864669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create function to handle the noise and variation in the location column\ndef consolidate_location(loc):\n    \"\"\"Maps common noise terms and aliases to standard forms.\"\"\"\n    if loc in ['unknown', '']:\n        return 'UNKNOWN'\n\n    # Common Noise/Junk terms identified during inspection\n    junk_list = ['worldwide', 'everywhere', 'here', 'the internet', 'my timeline', \n                 'noplace', 'follow me', 'he/him or she/her (ask)', 'email', \n                 'facebook', 'twitter', 'ig', 'snapchat', 'link in bio',\n                'five down from the coffeeshop', 'reddit', 'road to the billionaires club',\n                'all around the world', 'mad as hell', 'in the word of god',\n                'narnia', 'planet earth', 'ìït ,', 'ìït ,-','happily married with kids',\n                'america of founding fathers','taylor swift','theythem','anonymous','upstairs',\n                ',','httpwwwamazoncomdpbhr','in the shadows','international','the world',\n                'breaking news','in hell','pedophile hunting ground','neverland','world',\n                'world wide'] # this list could go on forever.  stopping here.\n    \n    if any(j in loc for j in junk_list):\n        return 'JUNK'\n\n    # Consolidate country/city variations\n    if loc in ['usa', 'united states', 'united states of america', 'us']:\n        return 'usa'\n    if loc in ['uk', 'london uk', 'england']:\n        return 'united kingdom'\n    if loc in ['new york', 'new york ny', 'ny', 'nyc']:\n        return 'new york city'\n    if loc in ['los angeles, ca', 'la']:\n        return 'los angeles'\n    if loc in ['ca', 'cali', 'california', 'southern california']:\n        return 'california'\n    if loc in ['texas', 'tx', 'republic of texas']:\n        return 'texas'\n    \n    return loc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:13.866220Z","iopub.execute_input":"2025-10-27T15:46:13.866550Z","iopub.status.idle":"2025-10-27T15:46:13.874573Z","shell.execute_reply.started":"2025-10-27T15:46:13.866523Z","shell.execute_reply":"2025-10-27T15:46:13.873814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# apply consolidated mapping\ndf_train['standard_location'] = df_train['clean_location'].apply(consolidate_location)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:13.875330Z","iopub.execute_input":"2025-10-27T15:46:13.875533Z","iopub.status.idle":"2025-10-27T15:46:13.922086Z","shell.execute_reply.started":"2025-10-27T15:46:13.875516Z","shell.execute_reply":"2025-10-27T15:46:13.921254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Aggregate low frequency locations\nTOP_N_LOCATIONS = 200\n\nlocation_counts = df_train['standard_location'].value_counts()\ntop_n_locations = location_counts.nlargest(TOP_N_LOCATIONS).index.tolist()\n\ndf_train['agg_location'] = np.where(\n    df_train['standard_location'].isin(top_n_locations),\n    df_train['standard_location'],\n    'UNKNOWN'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:13.922977Z","iopub.execute_input":"2025-10-27T15:46:13.923202Z","iopub.status.idle":"2025-10-27T15:46:13.942874Z","shell.execute_reply.started":"2025-10-27T15:46:13.923184Z","shell.execute_reply":"2025-10-27T15:46:13.942135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_unique_count = df_train['agg_location'].nunique()\nprint(f\"\\nAggregated to {final_unique_count} features.\")\nprint(f\"aggregated location feature unique counts:\")\nprint(df_train['agg_location'].value_counts().head(5))\n\n# Drop intermediate columns\ndf_train.drop(['location', 'clean_location', 'standard_location'], axis=1, inplace=True)\n\n#print(df_train['agg_location'].unique().tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:13.943960Z","iopub.execute_input":"2025-10-27T15:46:13.944849Z","iopub.status.idle":"2025-10-27T15:46:13.967155Z","shell.execute_reply.started":"2025-10-27T15:46:13.944817Z","shell.execute_reply":"2025-10-27T15:46:13.966130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"location_dummies = pd.get_dummies(df_train['agg_location'], prefix='agg_location')\n\ndf_train = pd.concat([df_train, location_dummies], axis =1)\n\ndf_train.drop('agg_location', axis =1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:13.968524Z","iopub.execute_input":"2025-10-27T15:46:13.968889Z","iopub.status.idle":"2025-10-27T15:46:13.991977Z","shell.execute_reply.started":"2025-10-27T15:46:13.968856Z","shell.execute_reply":"2025-10-27T15:46:13.991121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"New DataFrame Head (Showing New Location Columns)\")\nprint(df_train.head())\nprint(\"\\nNew DataFrame Shape\")\nprint(df_train.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:13.992953Z","iopub.execute_input":"2025-10-27T15:46:13.993155Z","iopub.status.idle":"2025-10-27T15:46:14.006088Z","shell.execute_reply.started":"2025-10-27T15:46:13.993139Z","shell.execute_reply":"2025-10-27T15:46:14.005143Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Architecture and Training\nDescribe model architecture and reasoning for why it is suitable for this problem.  \n\nInclude reference list for NLP-specific tutorials/discussion boards/code examples.  Methods to process texts to matrix form include: TF-IDF, GloVe, Word2Vec, etc.  Briefly explain the method and how they work.  \n\nBuild and train a sequential neural network model (any RNN family nn, including advanced architectures LSTM, GRU, bidirectional RNN, etc).","metadata":{}},{"cell_type":"markdown","source":"### Text Vectorization and Word Embedding\nThere are two components we'll use to transform the text: Keras's Tokenizer to convert the text to a sequence of integers and a Keras embedding layer for processing the text.  \n\nKeras's Tokenizer scans the entire 'clean_text', assigns a unique integer to every unique word, then converts each tweet into a sequence of those integers.\n\nThe Keras Embedding layer is initialized randomly and then learns a unique dense vector for every word in the vocabulary in training.  An advantage of this method vs GloVe or Word2Vec is that this process is simple and requires no external files.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Concatenate\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:14.006977Z","iopub.execute_input":"2025-10-27T15:46:14.007600Z","iopub.status.idle":"2025-10-27T15:46:17.516789Z","shell.execute_reply.started":"2025-10-27T15:46:14.007576Z","shell.execute_reply":"2025-10-27T15:46:17.515840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TEXT_COLUMN = 'clean_text'\nTARGET_COLUMN = 'target'\n\ncategorical_cols = [col for col in df_train.columns if col.startswith('location_') or col.startswith('keyword_')]\nX_cat = df_train[categorical_cols].values\nY = df_train[TARGET_COLUMN].values\n\nMAX_WORDS = 10000  # Only consider the top 10,000 words\nMAX_LEN = 50       # Pad/truncate sequences to a fixed length of 50\n\ntokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df_train[TEXT_COLUMN])\nword_index = tokenizer.word_index\n\n# Convert text to sequences and pad them\nsequences = tokenizer.texts_to_sequences(df_train[TEXT_COLUMN])\nX_text = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n\n# Split the dataset into training and validation sets\nX_text_train, X_text_val, X_cat_train, X_cat_val, Y_train, Y_val = train_test_split(\n    X_text, X_cat, Y, test_size=0.2, random_state=0, stratify=Y\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:17.517726Z","iopub.execute_input":"2025-10-27T15:46:17.518277Z","iopub.status.idle":"2025-10-27T15:46:17.743695Z","shell.execute_reply.started":"2025-10-27T15:46:17.518256Z","shell.execute_reply":"2025-10-27T15:46:17.742885Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Architecture\nWe're using a Bidirectional LSTM (Bi-LSTM) network.  \n\nLSTMs are a type of RNN designed to overcome the vanishing gradient problem, making them excellent at learning long term dependencies in sequence data (like text).\n\nBidirectional LSTM means that the tweet is processed twice.  One time running from beginning to end, and another from the end to the beginning.  The outputs are combined.\n\nFor text classification, knowing the context that follows a word can be as important as the context that precedes it and this method attempts to capture that.","metadata":{}},{"cell_type":"code","source":"# Build the Bi-LSTM Model Architecture\n\nEMBEDDING_DIM = 100\nVOCAB_SIZE = min(len(word_index) + 1, MAX_WORDS)\nNUM_CATEGORICAL_FEATURES = X_cat_train.shape[1] # Number of one-hot encoded features\n\n# Text Input Branch (RNN)\ntext_input = tf.keras.Input(shape=(MAX_LEN,), name='text_input')\nx = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(text_input)\nx = Bidirectional(LSTM(64, return_sequences=False))(x) # 64 units, returns only the final output\nx = Dense(32, activation='relu')(x)\nx = Dropout(0.5)(x)\ntext_output = x\n\n# Categorical Input Branch (Dense Network)\ncat_input = tf.keras.Input(shape=(NUM_CATEGORICAL_FEATURES,), name='cat_input')\ny = Dense(16, activation='relu')(cat_input) # Simple dense layer for categorical features\ncat_output = y\n\n# Merge Branches\n# Concatenate the output of the text branch and the categorical branch\nmerged = Concatenate()([text_output, cat_output])\n\n# Final Output Layer\nz = Dense(16, activation='relu')(merged)\nz = Dropout(0.5)(z)\noutput = Dense(1, activation='sigmoid')(z) # Sigmoid for binary classification\n\n# Create Model\nmodel = tf.keras.Model(inputs=[text_input, cat_input], outputs=output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:17.744533Z","iopub.execute_input":"2025-10-27T15:46:17.744818Z","iopub.status.idle":"2025-10-27T15:46:17.884505Z","shell.execute_reply.started":"2025-10-27T15:46:17.744798Z","shell.execute_reply":"2025-10-27T15:46:17.883585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameter Optimization & Training Setup\n\n# Optimization: Using a learning rate scheduler and dropout\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Start with a small learning rate\n\nmodel.compile(\n    optimizer=optimizer,\n    loss='binary_crossentropy',\n    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n)\n\n# Callbacks for improved training performance\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=5, restore_best_weights=True\n)\n\nprint(model.summary())\n\n# Train Model\nhistory = model.fit(\n     {'text_input': X_text_train, 'cat_input': X_cat_train},\n     Y_train,\n     epochs=20,\n     batch_size=32,\n     validation_data=({'text_input': X_text_val, 'cat_input': X_cat_val}, Y_val),\n     callbacks=[early_stopping],\n     verbose=1\n )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T15:46:17.885352Z","iopub.execute_input":"2025-10-27T15:46:17.885937Z","iopub.status.idle":"2025-10-27T15:47:39.651524Z","shell.execute_reply.started":"2025-10-27T15:46:17.885916Z","shell.execute_reply":"2025-10-27T15:47:39.650690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# apply preprocessing pipeline to the test data\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission_ids = df_test['id']\nprint(submission_ids.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T16:11:30.930181Z","iopub.execute_input":"2025-10-27T16:11:30.930586Z","iopub.status.idle":"2025-10-27T16:11:30.951535Z","shell.execute_reply.started":"2025-10-27T16:11:30.930563Z","shell.execute_reply":"2025-10-27T16:11:30.950560Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.series.Series'>\nRangeIndex: 3263 entries, 0 to 3262\nSeries name: id\nNon-Null Count  Dtype\n--------------  -----\n3263 non-null   int64\ndtypes: int64(1)\nmemory usage: 25.6 KB\nNone\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Describe the data size, dimension, structure, etc...\nprint(f\"Dimensions (Rows, Columns): {df_test.shape}\")\nprint(f\"Total elements: {df_test.size}\")\n\ndf_test.info()\nprint(\"\\nData Types per Column:\\n\", df_test.dtypes)\nprint(\"\\nFirst 3 rows:\\n\", df_test.head(3))\n\n# Check for missing values\nprint(\"\\nMissing values per column:\\n\", df_test.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T16:11:33.891590Z","iopub.execute_input":"2025-10-27T16:11:33.891932Z","iopub.status.idle":"2025-10-27T16:11:33.907200Z","shell.execute_reply.started":"2025-10-27T16:11:33.891908Z","shell.execute_reply":"2025-10-27T16:11:33.906110Z"}},"outputs":[{"name":"stdout","text":"Dimensions (Rows, Columns): (3263, 4)\nTotal elements: 13052\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        3263 non-null   int64 \n 1   keyword   3237 non-null   object\n 2   location  2158 non-null   object\n 3   text      3263 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 102.1+ KB\n\nData Types per Column:\n id           int64\nkeyword     object\nlocation    object\ntext        object\ndtype: object\n\nFirst 3 rows:\n    id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n\nMissing values per column:\n id             0\nkeyword       26\nlocation    1105\ntext           0\ndtype: int64\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Re-run all imputation and cleaning steps\n#df_test.dropna(subset=['keyword'], inplace=True)\ndf_test['keyword'].fillna('UNKNOWN', inplace=True)\ndf_test['location'].fillna('UNKNOWN', inplace=True)\ndf_test['clean_location'] = df_test['location'].apply(clean_location)\ndf_test['standard_location'] = df_test['clean_location'].apply(consolidate_location)\ndf_test['clean_keyword'] = df_test['keyword'].apply(unquote_keyword)\ndf_test['clean_text'] = df_test['text'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T16:12:04.739474Z","iopub.execute_input":"2025-10-27T16:12:04.739837Z","iopub.status.idle":"2025-10-27T16:12:05.324612Z","shell.execute_reply.started":"2025-10-27T16:12:04.739811Z","shell.execute_reply":"2025-10-27T16:12:05.323926Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_1058/2849685240.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_test['keyword'].fillna('UNKNOWN', inplace=True)\n/tmp/ipykernel_1058/2849685240.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_test['location'].fillna('UNKNOWN', inplace=True)\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# Aggregate Locations and Keywords\ndf_test['agg_location'] = np.where(\n    df_test['standard_location'].isin(top_n_locations),\n    df_test['standard_location'],\n    'OTHER_LOCATION'\n)\ndf_test['agg_keyword'] = np.where(\n    df_test['clean_keyword'].isin(top_n_keywords),\n    df_test['clean_keyword'],\n    'OTHER_KEYWORD'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T16:12:18.003257Z","iopub.execute_input":"2025-10-27T16:12:18.003562Z","iopub.status.idle":"2025-10-27T16:12:18.011451Z","shell.execute_reply.started":"2025-10-27T16:12:18.003540Z","shell.execute_reply":"2025-10-27T16:12:18.010411Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# One-Hot Encode Categorical Features\n# Create dummies for the test set\ntest_cat_dummies = pd.get_dummies(df_test['agg_location'], prefix='agg_location')\ntest_cat_dummies_k = pd.get_dummies(df_test['agg_keyword'], prefix='agg_keyword')\ntest_cat_dummies = pd.concat([test_cat_dummies, test_cat_dummies_k], axis=1)\n\n#print(test_cat_dummies.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T16:12:20.108405Z","iopub.execute_input":"2025-10-27T16:12:20.108735Z","iopub.status.idle":"2025-10-27T16:12:20.119018Z","shell.execute_reply.started":"2025-10-27T16:12:20.108712Z","shell.execute_reply":"2025-10-27T16:12:20.118094Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Reindex the test set dummy features to match the training set column list.\n# Columns in the test set but not in 'categorical_cols' are dropped (noise).\n# Columns in 'categorical_cols' but not in the test set are added as 0s (safe).\ntest_cat_aligned = test_cat_dummies.reindex(columns=categorical_cols, fill_value=0)\n\n# Convert the final, aligned DataFrame to a NumPy array for the Keras model\nX_cat_test = test_cat_aligned.values\n\nprint(f\"Categorical features successfully aligned. Final shape: {X_cat_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T16:12:22.202394Z","iopub.execute_input":"2025-10-27T16:12:22.203325Z","iopub.status.idle":"2025-10-27T16:12:22.210061Z","shell.execute_reply.started":"2025-10-27T16:12:22.203295Z","shell.execute_reply":"2025-10-27T16:12:22.209065Z"}},"outputs":[{"name":"stdout","text":"Categorical features successfully aligned. Final shape: (3263, 76)\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# Tokenize Text \nX_text_test = tokenizer.texts_to_sequences(df_test['clean_text'])\nX_text_test = pad_sequences(X_text_test, maxlen=MAX_LEN, padding='post', truncating='post')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T16:12:24.741189Z","iopub.execute_input":"2025-10-27T16:12:24.741494Z","iopub.status.idle":"2025-10-27T16:12:24.796977Z","shell.execute_reply.started":"2025-10-27T16:12:24.741473Z","shell.execute_reply":"2025-10-27T16:12:24.796127Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Make predictions (probabilities)\npredictions_proba = model.predict({'text_input': X_text_test, 'cat_input': X_cat_test})\n\n# Convert probabilities to binary class (0 or 1) using 0.5 threshold\npredictions = (predictions_proba > 0.5).astype(int).flatten()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T16:12:26.976801Z","iopub.execute_input":"2025-10-27T16:12:26.977121Z","iopub.status.idle":"2025-10-27T16:12:28.241693Z","shell.execute_reply.started":"2025-10-27T16:12:26.977100Z","shell.execute_reply":"2025-10-27T16:12:28.240742Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# Create Submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': submission_ids,\n    'target': predictions\n})\n\n# Display the head of the submission file\nprint(submission_df.head())\n\n# Save the submission file to CSV\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"\\nsubmission.csv created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T16:12:29.832022Z","iopub.execute_input":"2025-10-27T16:12:29.832621Z","iopub.status.idle":"2025-10-27T16:12:29.843928Z","shell.execute_reply.started":"2025-10-27T16:12:29.832592Z","shell.execute_reply":"2025-10-27T16:12:29.842941Z"}},"outputs":[{"name":"stdout","text":"   id  target\n0   0       1\n1   2       0\n2   3       1\n3   9       1\n4  11       1\n\nsubmission.csv created.\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"## Result and Analysis\nHyperparameter tuning, compare different architectures, apply techniques to improve training/performance.  Include results with tables and figures.  Discuss why or why not things worked well, any troubleshooting, and hyperparameter optimization procedure summary.","metadata":{}},{"cell_type":"markdown","source":"### Results \n#### (based on a recent 'Run' may not be the same results acheived on subsequent runs)\nBest Epoch = 6.  An accuracy of 81.3% suggests that the model is performing quite well at distinguishing between disaster and non-disaster tweets.  The difference between Precision (84.0%) and Recall (69.7%) indicates a slight bias.  The model correctly predicts a disaster 84% of the time, but misses about 30% of the actual disaster tweets (low recall).  \n\nThe F1 score is 2x(Precision * Recall)/(Precision + Recall) which is 76.2%.\n\n### Analysis\nWe used a Bi-LSTM instead of a simpler RNN to improve sequence learning capacity.  We used Adam optimizer with a small initial learning rate (1e^-4) for slow, stable convergence.  Dropout (0.5) was applied after the main Bi-LSTM output and before the final dense layers to fight overfitting.  We used an Early Stopping callback to monitor the val_loss.  Training automatically stopped after the validation loss failed to improve for 5 consecutive epochs.  This technique is used to prevent overfitting (and speed things up).  ","metadata":{}},{"cell_type":"markdown","source":"Submissions are evaluated using F1 between the predicted and expected answers.","metadata":{}},{"cell_type":"markdown","source":"## Discussion and Conclusion\nDiscuss and interpret results as well as learnings and takeaways.  What did and did not help improve the performance of the model(s)  What improvements could we try in the future?","metadata":{}},{"cell_type":"markdown","source":"### Areas for Improvement\nThe Bi-LSTM model might still struggle with stuble sarcasm or highly figurative language that is often found in social media.  \n\nA pre-trained embedding like GloVe might have gave a better starting point, especially if many words in the test set were not present in the training set.  \n\nThe categorical dense layers were simple, more complex interactions could be learned by adding more layers to the categorical branch. ","metadata":{}}]}